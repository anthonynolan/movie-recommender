{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/19 12:08:47 WARN Utils: Your hostname, DESKTOP-60CN0SA resolves to a loopback address: 127.0.1.1; using 172.29.46.194 instead (on interface eth0)\n",
      "23/02/19 12:08:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/19 12:08:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/19 12:08:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row, SparkSession\n",
    "spark = SparkSession.builder.appName(\"movie\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47d70e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RATINGS_FILE = \"../data/mllib/sample_movielens_data.txt\"\n",
    "RATINGS_FILE = \"ml-latest-small/ratings.csv\"\n",
    "MOVIES_FILE = \"ml-latest-small/movies.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bf496fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "3                         Comedy|Drama|Romance  \n",
       "4                                       Comedy  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "movies = pd.read_csv(MOVIES_FILE)\n",
    "movies[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "de5f3d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df = spark.createDataFrame(movies)\n",
    "movies_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ff81f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['userId,movieId,rating,timestamp\\n',\n",
       " '1,1,4.0,964982703\\n',\n",
       " '1,3,4.0,964981247\\n',\n",
       " '1,6,4.0,964982224\\n',\n",
       " '1,47,5.0,964983815\\n']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(RATINGS_FILE, 'rt') as f:\n",
    "    ratings = f.readlines()\n",
    "ratings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d9717ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('userId', 'int'),\n",
       " ('movieId', 'int'),\n",
       " ('rating', 'double'),\n",
       " ('timestamp', 'int')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('header', 'true').csv(RATINGS_FILE, inferSchema=True)\n",
    "print(df.count())\n",
    "display(df.dtypes)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91703df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS(maxIter=3, userCol='userId', itemCol='movieId', ratingCol='rating', coldStartStrategy='drop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d66ef34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(train, test) = df.randomSplit([.8, .2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f34b3e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:>                                                        (0 + 8) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/19 12:41:05 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = als.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5811e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5738d0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9124561115814143"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1259881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d1d4f560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('userId', 'int'),\n",
       " ('recommendations', 'array<struct<movieId:int,rating:float>>')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.recommendForAllUsers(1).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "84b024eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column '`recommendations[]`.movieId' does not exist. Did you mean one of the following? [recommendations, userId];\n'Project ['recommendations[].movieId]\n+- Project [id#4648 AS userId#4652, cast(recommendations#4649 as array<struct<movieId:int,rating:float>>) AS recommendations#4653]\n   +- Project [key#4642 AS id#4648, TopByKeyAggregator(scala.Tuple3)#4647 AS recommendations#4649]\n      +- Aggregate [value#4634], [value#4634 AS key#4642, topbykeyaggregator(org.apache.spark.ml.recommendation.TopByKeyAggregator@515c3340, Some(newInstance(class scala.Tuple3)), Some(class scala.Tuple3), Some(StructType(StructField(_1,IntegerType,false),StructField(_2,IntegerType,false),StructField(_3,FloatType,false))), encodeusingserializer(input[0, java.lang.Object, true], true), decodeusingserializer(input[0, binary, true], org.apache.spark.util.BoundedPriorityQueue, true), mapobjects(lambdavariable(MapObject, ObjectType(class scala.Tuple2), true, 451), if (isnull(lambdavariable(MapObject, ObjectType(class scala.Tuple2), true, 451))) null else named_struct(_1, knownnotnull(lambdavariable(MapObject, ObjectType(class scala.Tuple2), true, 451))._1, _2, knownnotnull(lambdavariable(MapObject, ObjectType(class scala.Tuple2), true, 451))._2), input[0, [Lscala.Tuple2;, true], None), ArrayType(StructType(StructField(_1,IntegerType,false),StructField(_2,FloatType,false)),true), true, 0, 0) AS TopByKeyAggregator(scala.Tuple3)#4647]\n         +- AppendColumns org.apache.spark.ml.recommendation.ALSModel$$Lambda$4280/0x00000001017d1040@3efdf5ea, class scala.Tuple3, [StructField(_1,IntegerType,false), StructField(_2,IntegerType,false), StructField(_3,FloatType,false)], newInstance(class scala.Tuple3), [input[0, int, false] AS value#4634]\n            +- SerializeFromObject [knownnotnull(assertnotnull(input[0, scala.Tuple3, true]))._1 AS _1#4623, knownnotnull(assertnotnull(input[0, scala.Tuple3, true]))._2 AS _2#4624, knownnotnull(assertnotnull(input[0, scala.Tuple3, true]))._3 AS _3#4625]\n               +- MapPartitions org.apache.spark.ml.recommendation.ALSModel$$Lambda$4278/0x00000001017cf840@3970cef9, obj#4622: scala.Tuple3\n                  +- DeserializeToObject newInstance(class scala.Tuple4), obj#4621: scala.Tuple4\n                     +- Join Cross\n                        :- SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(IntegerType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1, true, false, true) AS _1#4592, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, true, false, true) AS _2#4593]\n                        :  +- MapPartitions org.apache.spark.ml.recommendation.ALSModel$$Lambda$4275/0x00000001017ce040@4ee2674d, obj#4591: scala.Tuple2\n                        :     +- DeserializeToObject newInstance(class scala.Tuple2), obj#4590: scala.Tuple2\n                        :        +- Project [_1#484 AS id#489, _2#485 AS features#490]\n                        :           +- SerializeFromObject [knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1 AS _1#484, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, true, false, true) AS _2#485]\n                        :              +- ExternalRDD [obj#483]\n                        +- SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(IntegerType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1, true, false, true) AS _1#4603, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, true, false, true) AS _2#4604]\n                           +- MapPartitions org.apache.spark.ml.recommendation.ALSModel$$Lambda$4275/0x00000001017ce040@78f8d4fd, obj#4602: scala.Tuple2\n                              +- DeserializeToObject newInstance(class scala.Tuple2), obj#4601: scala.Tuple2\n                                 +- Project [_1#496 AS id#501, _2#497 AS features#502]\n                                    +- SerializeFromObject [knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1 AS _1#496, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, true, false, true) AS _2#497]\n                                       +- ExternalRDD [obj#495]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4283/2881644143.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecommendForAllUsers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recommendations[].movieId\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2021\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m         \"\"\"\n\u001b[0;32m-> 2023\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2024\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column '`recommendations[]`.movieId' does not exist. Did you mean one of the following? [recommendations, userId];\n'Project ['recommendations[].movieId]\n+- Project [id#4648 AS userId#4652, cast(recommendations#4649 as array<struct<movieId:int,rating:float>>) AS recommendations#4653]\n   +- Project [key#4642 AS id#4648, TopByKeyAggregator(scala.Tuple3)#4647 AS recommendations#4649]\n      +- Aggregate [value#4634], [value#4634 AS key#4642, topbykeyaggregator(org.apache.spark.ml.recommendation.TopByKeyAggregator@515c3340, Some(newInstance(class scala.Tuple3)), Some(class scala.Tuple3), Some(StructType(StructField(_1,IntegerType,false),StructField(_2,IntegerType,false),StructField(_3,FloatType,false))), encodeusingserializer(input[0, java.lang.Object, true], true), decodeusingserializer(input[0, binary, true], org.apache.spark.util.BoundedPriorityQueue, true), mapobjects(lambdavariable(MapObject, ObjectType(class scala.Tuple2), true, 451), if (isnull(lambdavariable(MapObject, ObjectType(class scala.Tuple2), true, 451))) null else named_struct(_1, knownnotnull(lambdavariable(MapObject, ObjectType(class scala.Tuple2), true, 451))._1, _2, knownnotnull(lambdavariable(MapObject, ObjectType(class scala.Tuple2), true, 451))._2), input[0, [Lscala.Tuple2;, true], None), ArrayType(StructType(StructField(_1,IntegerType,false),StructField(_2,FloatType,false)),true), true, 0, 0) AS TopByKeyAggregator(scala.Tuple3)#4647]\n         +- AppendColumns org.apache.spark.ml.recommendation.ALSModel$$Lambda$4280/0x00000001017d1040@3efdf5ea, class scala.Tuple3, [StructField(_1,IntegerType,false), StructField(_2,IntegerType,false), StructField(_3,FloatType,false)], newInstance(class scala.Tuple3), [input[0, int, false] AS value#4634]\n            +- SerializeFromObject [knownnotnull(assertnotnull(input[0, scala.Tuple3, true]))._1 AS _1#4623, knownnotnull(assertnotnull(input[0, scala.Tuple3, true]))._2 AS _2#4624, knownnotnull(assertnotnull(input[0, scala.Tuple3, true]))._3 AS _3#4625]\n               +- MapPartitions org.apache.spark.ml.recommendation.ALSModel$$Lambda$4278/0x00000001017cf840@3970cef9, obj#4622: scala.Tuple3\n                  +- DeserializeToObject newInstance(class scala.Tuple4), obj#4621: scala.Tuple4\n                     +- Join Cross\n                        :- SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(IntegerType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1, true, false, true) AS _1#4592, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, true, false, true) AS _2#4593]\n                        :  +- MapPartitions org.apache.spark.ml.recommendation.ALSModel$$Lambda$4275/0x00000001017ce040@4ee2674d, obj#4591: scala.Tuple2\n                        :     +- DeserializeToObject newInstance(class scala.Tuple2), obj#4590: scala.Tuple2\n                        :        +- Project [_1#484 AS id#489, _2#485 AS features#490]\n                        :           +- SerializeFromObject [knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1 AS _1#484, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, true, false, true) AS _2#485]\n                        :              +- ExternalRDD [obj#483]\n                        +- SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(IntegerType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1, true, false, true) AS _1#4603, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, true, false, true) AS _2#4604]\n                           +- MapPartitions org.apache.spark.ml.recommendation.ALSModel$$Lambda$4275/0x00000001017ce040@78f8d4fd, obj#4602: scala.Tuple2\n                              +- DeserializeToObject newInstance(class scala.Tuple2), obj#4601: scala.Tuple2\n                                 +- Project [_1#496 AS id#501, _2#497 AS features#502]\n                                    +- SerializeFromObject [knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1 AS _1#496, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, true, false, true) AS _2#497]\n                                       +- ExternalRDD [obj#495]\n"
     ]
    }
   ],
   "source": [
    "model.recommendForAllUsers(1).select(F.col(\"recommendations[].movieId\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d1906c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 420:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|    720|Wallace & Gromit:...|Adventure|Animati...|\n",
      "|    720|Wallace & Gromit:...|Adventure|Animati...|\n",
      "|    720|Wallace & Gromit:...|Adventure|Animati...|\n",
      "|   1280|Raise the Red Lan...|               Drama|\n",
      "|   1241|Dead Alive (Brain...|Comedy|Fantasy|Ho...|\n",
      "|    932|Affair to Remembe...|       Drama|Romance|\n",
      "|    932|Affair to Remembe...|       Drama|Romance|\n",
      "|    417|    Barcelona (1994)|      Comedy|Romance|\n",
      "|    232|Eat Drink Man Wom...|Comedy|Drama|Romance|\n",
      "|    232|Eat Drink Man Wom...|Comedy|Drama|Romance|\n",
      "|    446|Farewell My Concu...|       Drama|Romance|\n",
      "|     28|   Persuasion (1995)|       Drama|Romance|\n",
      "|     28|   Persuasion (1995)|       Drama|Romance|\n",
      "|     28|   Persuasion (1995)|       Drama|Romance|\n",
      "|     28|   Persuasion (1995)|       Drama|Romance|\n",
      "|     28|   Persuasion (1995)|       Drama|Romance|\n",
      "|     28|   Persuasion (1995)|       Drama|Romance|\n",
      "|     28|   Persuasion (1995)|       Drama|Romance|\n",
      "|     28|   Persuasion (1995)|       Drama|Romance|\n",
      "|     28|   Persuasion (1995)|       Drama|Romance|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model.recommendForAllUsers(1).select(F.element_at(F.col(\"recommendations.movieId\"),1).alias('movieId')).join(movies_df, on='movieId').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2c9a3725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('userId', 'int'),\n",
       " ('recommendations', 'array<struct<movieId:int,rating:float>>')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.recommendForAllUsers(1).dtypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
